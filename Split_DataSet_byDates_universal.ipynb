{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05bc03f8",
   "metadata": {},
   "source": [
    "A simple tool to split the InsectSound1000 dataset into train, validation and test subsets while making sure the recording dates of the three subsets don't overlap. \n",
    "\n",
    "Branding et al. (2023), Scientific Data, InsectSound1000 An Insect Sound Dataset for Deep Learning based Acoustic Insect Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a9c537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move or copy files?\n",
    "copy_or_move = 'copy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc5a016b-3c91-465e-a9e6-7de4766d8ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# set path to input directory containing all files:\n",
    "input_dir = 'D:/InsectSound1000'\n",
    "\n",
    "# Set path to output directory:\n",
    "base_dir = 'D:/InsectSound1000'\n",
    "\n",
    "\n",
    "labels = ['Coccinella',\n",
    "          'Episyrphus',\n",
    "          'Bombus',\n",
    "          'Rhaphigaster',\n",
    "          'Bradysia',\n",
    "          'Aphidoletes', \n",
    "          'Halyomorpha',\n",
    "          'Nezara',\n",
    "          'Palomena',\n",
    "          'Trialeurodes',\n",
    "          'Myzus',\n",
    "          'Tuta']\n",
    "\n",
    "# Directories for our training,\n",
    "# validation and test splits\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "test_dir = os.path.join(base_dir, 'test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c547a85f-5e66-4039-94d7-0d7ad0dad1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create split directories:\n",
    "os.mkdir(train_dir)\n",
    "os.mkdir(validation_dir)\n",
    "os.mkdir(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df14639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from fetchfiles import fetchfiles\n",
    "\n",
    "for label in labels:\n",
    "    this_label_files = fetchfiles(input_dir, label)\n",
    "    print(str(len(this_label_files)) + ' files found for ' + label)\n",
    "\n",
    "    # get all recording dates:\n",
    "    dates = []\n",
    "    for file in this_label_files:\n",
    "        _, fname = os.path.split(file)\n",
    "        date = fname[:8]\n",
    "        if date not in dates:\n",
    "            dates.append(date)\n",
    "\n",
    "    # use list compression to build a dates dictionary for this label:\n",
    "    dates_dict = {}\n",
    "    for date in dates:\n",
    "        dates_dict[date] = [file for file in this_label_files if date in file]\n",
    "        \n",
    "    # sort by length of list:\n",
    "    dates_dict = dict(sorted(dates_dict.items(), key=lambda item: len(item[1]), reverse=True))\n",
    "    # print sorted dict:\n",
    "    for key, value in dates_dict.items():\n",
    "        print(key, len(value))\n",
    "    \n",
    "    # now, let's split the files into train, validation and test sets:\n",
    "    # make sure dates don't overlap between sets:\n",
    "    train_files = []\n",
    "    validation_files = []\n",
    "    test_files = []\n",
    "    \n",
    "    if len(dates_dict) < 3:\n",
    "        print('Not enough dates to split the data into train, validation and test sets.')\n",
    "        break\n",
    "    else:\n",
    "        # biggest date goes to train, second biggest to validation, third biggest to test\n",
    "        # rest gets added to train:\n",
    "         # put the big files in first:\n",
    "        train_files.extend(dates_dict.pop(next(iter(dates_dict))))\n",
    "        validation_files.extend(dates_dict.pop(next(iter(dates_dict))))\n",
    "        test_files.extend(dates_dict.pop(next(iter(dates_dict))))\n",
    "        \n",
    "        while len(dates_dict) > 0:\n",
    "        \n",
    "            # but make sure we have at least 1000 samples in the test set:\n",
    "            while(len(test_files)/len(this_label_files)) < 0.15:\n",
    "                if len(dates_dict) == 0:\n",
    "                    break\n",
    "                # add the smallest date to the test set:\n",
    "                test_files.extend(dates_dict.pop(next(reversed(dates_dict))))\n",
    "                \n",
    "            # but make sure the validation set is not to small:\n",
    "            while (len(validation_files)/len(this_label_files)) < 0.15:\n",
    "                if len(dates_dict) == 0:\n",
    "                    break\n",
    "                # add the smallest left date to validation set:\n",
    "                validation_files.extend(dates_dict.pop(next(reversed(dates_dict))))\n",
    "            \n",
    "            # add the rest to the train set:\n",
    "            if len(dates_dict) > 0:\n",
    "                train_files.extend(dates_dict.pop(next(reversed(dates_dict))))\n",
    "        \n",
    "    print('%s %s %s' % (str(len(train_files)), str(len(validation_files)), str(len(test_files))))\n",
    "    print(str(round(len(train_files)/len(this_label_files)*100, 2)) + '% ' \n",
    "          + str(round(len(validation_files)/len(this_label_files)*100, 2)) + '% ' \n",
    "          + str(round(len(test_files)/len(this_label_files)*100, 2)) + '%')\n",
    "    print()  \n",
    "\n",
    "    # now, let's copy the files to the new directories:\n",
    "    for files, dst_folder in zip([train_files, validation_files, test_files], [train_dir, validation_dir, test_dir]):\n",
    "        for file in files:\n",
    "            dontneedthis, fname = os.path.split(file)\n",
    "            src = file\n",
    "            dst = os.path.join(dst_folder, fname)\n",
    "            if copy_or_move == 'move':\n",
    "                shutil.move(src, dst)\n",
    "            elif copy_or_move == 'copy':\n",
    "                shutil.copy(src, dst)     \n",
    "       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Branding_SignalProcessing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
